{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95935417-3a67-49c3-b863-f222b0795db6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from math import pi\n",
    "from pyspark.sql import functions as F, Row\n",
    "import os\n",
    "import shutil\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "import gc\n",
    "import logging\n",
    "\n",
    "logging.getLogger(\"root\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a993fd57-9c73-47d8-9e60-82109520a441",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Number of samples\n",
    "# num_samples = 25000\n",
    "\n",
    "# # Create DataFrame with two independent uniform random columns\n",
    "# df = (\n",
    "#     spark.range(num_samples)\n",
    "#     .withColumn(\"u1\", F.rand(seed=42))  # Uniform for radial distance\n",
    "#     .withColumn(\"u2\", F.rand(seed=84))  # Uniform for angle\n",
    "# )\n",
    "\n",
    "# # Corrected radial coordinate calculation (density proportional to r)\n",
    "# df = (\n",
    "#     df.withColumn(\"r\", F.sqrt(\"u1\"))  # Inverse transform for density ~ r\n",
    "#     .withColumn(\"theta\", F.lit(2) * F.pi() * F.col(\"u2\"))\n",
    "#     .withColumn(\"abcissa\", F.col(\"r\") * F.cos(\"theta\"))\n",
    "#     .withColumn(\"ordinate\", F.col(\"r\") * F.sin(\"theta\"))\n",
    "#     .withColumn(\"y\", F.col(\"r\"))\n",
    "# )\n",
    "\n",
    "# # Show results\n",
    "# df.limit(10).display()\n",
    "\n",
    "# # Save this data to table\n",
    "# df.write.saveAsTable(\"default.polar_coordinates_data\")\n",
    "\n",
    "# # Optional: Visual verification would show increased density near center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf4af16d-0e32-4241-9417-0ba4e4673db5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = spark.table(\"default.polar_coordinates_data\")\n",
    "\n",
    "# Define the angular sector range (e.g., between pi/6 and pi/3)\n",
    "theta_min = pi / 6\n",
    "theta_max = pi / 3\n",
    "\n",
    "# Define the radial range\n",
    "radius_min = 0.0\n",
    "radius_max = 1.0\n",
    "\n",
    "\n",
    "# Collect the data\n",
    "def sector_filter(theta_col, r_col, theta_min, theta_max, radius_min, radius_max):\n",
    "    return (\n",
    "        (theta_col >= theta_min)\n",
    "        & (theta_col <= theta_max)\n",
    "        & (r_col >= radius_min)\n",
    "        & (r_col <= radius_max)\n",
    "    )\n",
    "\n",
    "\n",
    "training_data_all = (\n",
    "    df.where(\n",
    "        ~sector_filter(\n",
    "            F.col(\"theta\"), F.col(\"r\"), theta_min, theta_max, radius_min, radius_max\n",
    "        )\n",
    "    )\n",
    "    .select(\"abcissa\", \"ordinate\", \"y\")\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "train_data, val_data = train_test_split(\n",
    "    training_data_all, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "test_data = (\n",
    "    df.where(\n",
    "        sector_filter(\n",
    "            F.col(\"theta\"), F.col(\"r\"), theta_min, theta_max, radius_min, radius_max\n",
    "        )\n",
    "    )\n",
    "    .select(\"abcissa\", \"ordinate\", \"y\")\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "258e196b-188b-485f-bf47-d0eb775d08aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class SparkDataFrameDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        spark_df,\n",
    "        x1=\"abcissa\",\n",
    "        x2=\"ordinate\",\n",
    "        target_col=\"y\",\n",
    "        polar_coordinates=False,\n",
    "    ):\n",
    "        data = spark_df.select(x1, x2, target_col).toPandas()\n",
    "        x = data[x1].values\n",
    "        y = data[x2].values\n",
    "        output = data[target_col].values\n",
    "        if polar_coordinates:\n",
    "            r = torch.sqrt(torch.tensor(x) ** 2 + torch.tensor(y) ** 2)\n",
    "            theta = torch.atan2(torch.tensor(y), torch.tensor(x))\n",
    "            self.data = torch.stack((r, theta), dim=1).float()\n",
    "        else:\n",
    "            self.data = torch.tensor(list(zip(x, y)), dtype=torch.float32)\n",
    "        self.targets = torch.tensor(output, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "\n",
    "df_train, df_val = df.where(\n",
    "    ~sector_filter(\n",
    "        F.col(\"theta\"), F.col(\"r\"), theta_min, theta_max, radius_min, radius_max\n",
    "    )\n",
    ").randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "\n",
    "df_test = df.where(\n",
    "    sector_filter(\n",
    "        F.col(\"theta\"), F.col(\"r\"), theta_min, theta_max, radius_min, radius_max\n",
    "    )\n",
    ")\n",
    "\n",
    "print(df_train.count(), df_val.count(), df_test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c573cfa7-e88e-482f-a28e-809411b0e182",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract x, y, and output values for training data\n",
    "x_values_train = df_train.select(\"abcissa\").toPandas()[\"abcissa\"].tolist()\n",
    "y_values_train = df_train.select(\"ordinate\").toPandas()[\"ordinate\"].tolist()\n",
    "output_values_train = df_train.select(\"y\").toPandas()[\"y\"].tolist()\n",
    "\n",
    "x_values_val = df_val.select(\"abcissa\").toPandas()[\"abcissa\"].tolist()\n",
    "y_values_val = df_val.select(\"ordinate\").toPandas()[\"ordinate\"].tolist()\n",
    "output_values_val = df_val.select(\"y\").toPandas()[\"y\"].tolist()\n",
    "\n",
    "# Extract x, y values for test data\n",
    "x_values_test = df_test.select(\"abcissa\").toPandas()[\"abcissa\"].tolist()\n",
    "y_values_test = df_test.select(\"ordinate\").toPandas()[\"ordinate\"].tolist()\n",
    "\n",
    "# Plot the data with color by output\n",
    "plt.figure(figsize=(8, 8), dpi=256)\n",
    "plt.scatter(\n",
    "    x_values_train,\n",
    "    y_values_train,\n",
    "    c=output_values_train,\n",
    "    cmap=\"twilight\",\n",
    "    alpha=1,\n",
    "    s=0.5,\n",
    "    label=\"Training Data\",\n",
    ")\n",
    "plt.scatter(\n",
    "    x_values_test, y_values_test, color=\"red\", alpha=1, s=0.5, label=\"Test Data\"\n",
    ")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"2D Scatter Plot of x and y (colored by output)\")\n",
    "plt.colorbar(label=\"output\")\n",
    "plt.grid(True)\n",
    "plt.gca().set_aspect(\"equal\", adjustable=\"box\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3555201d-8546-4f23-a356-343a94c4b087",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class RegressorLinearSingleLayer(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(RegressorLinearSingleLayer, self).__init__()\n",
    "        self.model = nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.MSELoss()(y_hat, y)\n",
    "        self.log(\"train_loss\", loss, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.MSELoss()(y_hat, y)\n",
    "        self.log(\"val_loss\", loss, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "class RegressorLinearMultiLayer(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(RegressorLinearMultiLayer, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(2, 64), nn.ReLU(), nn.Linear(64, 32), nn.ReLU(), nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.MSELoss()(y_hat, y)\n",
    "        self.log(\"train_loss\", loss, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.MSELoss()(y_hat, y)\n",
    "        self.log(\"val_loss\", loss, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "def train_model(model_parameters, dataset_train, dataset_val, dataset_test):\n",
    "    model = model_parameters[\"model\"]\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=model_parameters[\"max_epochs\"],\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        callbacks=[checkpoint_callback],\n",
    "    )\n",
    "    trainer.fit(\n",
    "        model=model, train_dataloaders=dataloader_train, val_dataloaders=dataloader_val\n",
    "    )\n",
    "\n",
    "\n",
    "def get_epoch_predictions(model, model_name, dataloader_test):\n",
    "    checkpoint_dir = f\"/Workspace/Shared/lightning_logs/{model_name}/checkpoints\"\n",
    "    checkpoints = [f for f in os.listdir(checkpoint_dir) if f.endswith(\".ckpt\")]\n",
    "\n",
    "    epoch_predictions = []\n",
    "\n",
    "    for checkpoint in checkpoints:\n",
    "        epoch_num = int(checkpoint.split(\".\")[0].split(\"=\")[-1].split(\".\")[0])\n",
    "        checkpoint_data = torch.load(os.path.join(checkpoint_dir, checkpoint))\n",
    "        model.load_state_dict(checkpoint_data[\"state_dict\"], strict=False)\n",
    "        model.eval()\n",
    "        predictions = []\n",
    "        for batch in dataloader_test:\n",
    "            x, _ = batch\n",
    "            with torch.no_grad():\n",
    "                y_hat = model(x)\n",
    "            predictions.append(y_hat)\n",
    "        preds_np = torch.cat(predictions).numpy().flatten()\n",
    "        rows = [Row(epoch=epoch_num, prediction=float(pred)) for pred in preds_np]\n",
    "        df_epoch = spark.createDataFrame(rows)\n",
    "        epoch_predictions.append(df_epoch)\n",
    "\n",
    "    df_all_epochs = epoch_predictions[0]\n",
    "    for df in epoch_predictions[1:]:\n",
    "        df_all_epochs = df_all_epochs.union(df)\n",
    "\n",
    "    display(df_all_epochs.limit(10))\n",
    "    return df_all_epochs\n",
    "\n",
    "\n",
    "def plot_and_save_predictions_by_epoch(\n",
    "    epochs,\n",
    "    preds_by_epoch,\n",
    "    x_values_train,\n",
    "    y_values_train,\n",
    "    output_values_train,\n",
    "    x_values_test,\n",
    "    y_values_test,\n",
    "    path_plot_dir,\n",
    "    model_name,\n",
    "):\n",
    "    for epoch in epochs:\n",
    "        test_predictions = preds_by_epoch[epoch]\n",
    "        plt.figure(figsize=(8, 8), dpi=256)\n",
    "        plt.scatter(\n",
    "            x_values_train,\n",
    "            y_values_train,\n",
    "            c=output_values_train,\n",
    "            cmap=\"twilight\",\n",
    "            alpha=1,\n",
    "            s=0.5,\n",
    "            label=\"Train Data\",\n",
    "        )\n",
    "        plt.scatter(\n",
    "            x_values_test,\n",
    "            y_values_test,\n",
    "            c=test_predictions,\n",
    "            cmap=\"twilight\",\n",
    "            alpha=1,\n",
    "            s=0.5,\n",
    "            label=\"Test Predictions\",\n",
    "        )\n",
    "        plt.xlabel(\"x\")\n",
    "        plt.ylabel(\"y\")\n",
    "        plt.title(f\"2D Scatter Plot of x and y (colored by output) - Epoch {epoch}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.gca().set_aspect(\"equal\", adjustable=\"box\")\n",
    "\n",
    "        fig = plt.gcf()\n",
    "        with open(\n",
    "            f\"{path_plot_dir}/{model_name}_Plot_epoch={epoch:03d}.pkl\", \"wb\"\n",
    "        ) as f:\n",
    "            pickle.dump(fig, f)\n",
    "\n",
    "        plt.savefig(\n",
    "            f\"{path_plot_dir}/{model_name}_Plot_epoch={epoch:03d}.png\",\n",
    "            bbox_inches=\"tight\",\n",
    "            pad_inches=0,\n",
    "        )\n",
    "        pd.DataFrame(\n",
    "            {\"x\": x_values_test, \"y\": y_values_test, \"prediction\": test_predictions}\n",
    "        ).to_csv(\n",
    "            f\"{path_plot_dir}/{model_name}_Data_epoch={epoch:03d}.csv\", index=False\n",
    "        )\n",
    "        plt.close()\n",
    "        del test_predictions\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "def display_prediction_images(path_plot_dir):\n",
    "    png_files = [f for f in os.listdir(path_plot_dir) if f.endswith(\".png\")]\n",
    "    images = [Image.open(os.path.join(path_plot_dir, f)) for f in sorted(png_files)]\n",
    "\n",
    "    grid_size = int(len(images) ** 0.5) + 1\n",
    "\n",
    "    fig, axes = plt.subplots(grid_size, grid_size, figsize=(20, 20))\n",
    "    for ax, img in zip(axes.flatten(), images):\n",
    "        ax.imshow(img)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    for ax in axes.flatten()[len(images) :]:\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    output_path = os.path.join(path_plot_dir, \"grid_image.png\")\n",
    "    plt.savefig(output_path, bbox_inches=\"tight\", pad_inches=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c03107d-c0e7-42b9-a20b-502267e7d619",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for model_type in [\"RegressorLinearSingleLayer\", \"RegressorLinearMultiLayer\"]:\n",
    "    for polar_coordinates in [True, False]:\n",
    "        model_parameters = {\n",
    "            \"model\": globals()[model_type](),\n",
    "            \"iteration\": 1,\n",
    "            \"polar_coordinates\": polar_coordinates,\n",
    "            \"max_epochs\": 20,\n",
    "        }\n",
    "\n",
    "\n",
    "        model_name = f\"{model_parameters['model'].__class__.__name__}_PolarCoordinates_{model_parameters['polar_coordinates']}_{model_parameters['iteration']:03d}\"\n",
    "\n",
    "\n",
    "        checkpoint_dir = f\"/Workspace/Shared/lightning_logs/{model_name}/checkpoints/\"\n",
    "        if os.path.exists(checkpoint_dir):\n",
    "            shutil.rmtree(checkpoint_dir)\n",
    "\n",
    "        checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "            dirpath=f\"/Workspace/Shared/lightning_logs/{model_name}/checkpoints/\",\n",
    "            filename=f\"{model_name}_{{epoch:03d}}\",\n",
    "            save_top_k=-1,\n",
    "            save_weights_only=True,\n",
    "            every_n_epochs=1,\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"min\",\n",
    "        )\n",
    "\n",
    "\n",
    "        dataset_train = SparkDataFrameDataset(\n",
    "            df_train, polar_coordinates=model_parameters[\"polar_coordinates\"]\n",
    "        )\n",
    "        dataset_val = SparkDataFrameDataset(\n",
    "            df_val, polar_coordinates=model_parameters[\"polar_coordinates\"]\n",
    "        )\n",
    "        dataset_test = SparkDataFrameDataset(\n",
    "            df_test, polar_coordinates=model_parameters[\"polar_coordinates\"]\n",
    "        )\n",
    "\n",
    "        dataloader_train = DataLoader(dataset_train, batch_size=256, shuffle=True)\n",
    "        dataloader_val = DataLoader(dataset_val, batch_size=256)\n",
    "        dataloader_test = DataLoader(dataset_test, batch_size=256)\n",
    "\n",
    "        path_plot_dir = f\"//Workspace/Shared/lightning_logs/{model_name}/plots/\"\n",
    "        os.makedirs(path_plot_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "        train_model(model_parameters, dataset_train, dataset_val, dataset_test)\n",
    "\n",
    "\n",
    "        df_all_epochs = get_epoch_predictions(\n",
    "            model_parameters[\"model\"], model_name, dataloader_test\n",
    "        )\n",
    "        epochs = [row[\"epoch\"] for row in df_all_epochs.select(\"epoch\").distinct().collect()]\n",
    "        epochs = sorted(epochs)\n",
    "\n",
    "        df_all_epochs = df_all_epochs.orderBy(\"epoch\").toPandas()\n",
    "\n",
    "\n",
    "        preds_by_epoch = {\n",
    "            epoch: df_all_epochs[df_all_epochs[\"epoch\"] == epoch][\"prediction\"].values\n",
    "            for epoch in epochs\n",
    "        }\n",
    "\n",
    "\n",
    "        plot_and_save_predictions_by_epoch(\n",
    "            epochs,\n",
    "            preds_by_epoch,\n",
    "            x_values_train,\n",
    "            y_values_train,\n",
    "            output_values_train,\n",
    "            x_values_test,\n",
    "            y_values_test,\n",
    "            path_plot_dir,\n",
    "            model_name,\n",
    "        )\n",
    "\n",
    "\n",
    "        display_prediction_images(path_plot_dir)\n",
    "        print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "629d44aa-8479-43b4-a9c3-23f761f32477",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "torch",
     "pytorch-lightning"
    ],
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "MachineLearning_Fermibot_2DColorRegressor",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
